---
title: "Effect of Irrepresentability Condition on Horseshoe"
author: "Jyotishka Datta"
date: "December 3, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = T)
```

```{R, echo = F}
library(glmnet)
library(monomvn)
library(bayesm)
library(plyr)
library(lars)
library(horseshoe)
library("parallel")
library("foreach")
library("doParallel")
```

## Background

The optimality properties of Lasso are well-known and they depend on "neighbourhood stability'' or "irrepresentability'' condition
and ``beta-min'' condition. Informally, these conditions guarantee against ill-posed design matrix and separability of signal and noise parameters. We show here a small simulation study repeated from Datta and Ghosh (2015) to show that the effect of 'irrepresentability condition' is not as strong on the Horseshoe variable selection. 

## What is irrepresentability and how it affects Lasso

We describe the ``irrepresentable" condition below:\par
Suppose, the sample covariance matrix is denoted by $\hat{\Sigma} = nX^T X$ and the active-set $S_0 = \{ j : \beta_j \neq 0\}$ consists of first $s_0$ elements of $\beta$. One can partition the $\hat{\Sigma}$ matrix as 

$$
\hat{\Sigma} = \left(\begin{array}{cc}    
\hat{\Sigma}_{s_0,s_0} & \hat{\Sigma}_{s_0,p-s_0}\\
\hat{\Sigma}_{p-s_0,s_0} & \hat{\Sigma}_{p-s_0,p-s_0}
\end{array} \right) 
$$

where $\hat{\Sigma}_{s_0,s_0}$ is a $s_0\times s_0$ matrix corresponding to the active variables and so on. The irrepresentable condition for variable selection consistency of Lasso is:

$$
|| \hat{\Sigma}_{p-s_0,s_0} \hat{\Sigma}_{s_0,s_0}^{-1} sign(\beta_{S_0}) ||_{\infty} \leq \theta \quad \mbox{ for some } 0 < \theta < 1
$$

This condition is sufficient and almost necessary in the sense that the necessary condition is only slightly weaker than the sufficient condition. The necssary condition requires '$\leq 1$', while the sufficient condition involves $\leq \theta$ for some $0 < \theta < 1$. The irrepresentable condition fails to hold if the design matrix is too ill-posed, i.e. has multi-collinearity. 

Buhlmann (2011) warn the readers that the irrepresentable condition may fail even though the design matrix is not ill-posed and it might restrict what can be done in high-dimensional problems. Zhao et al. (2006) provide numerical example to show the effect of the irrepresentable condition on the variable selection performance of Lasso. They showed that the probability of selecting the true sparse model is an increasing function of the irrepresentability condition number, defined as 
$$
\eta_{\infty} = 1 - || \hat{\Sigma}_{p-s_0,s_0} \hat{\Sigma}_{s_0,s_0}^{-1} sign(\beta_{S_0}) ||_{\infty}
$$

In particular, the probability of Lasso selecting the true model is almost 1 when $n_{\infty} > 0.2$ and it is almost zero when $\eta_{\infty} < -0.3$.


## Simulation

We simulated data with $n = 100, p = 32$ and $s_0 = 5$ with the sparse coefficient vector $\beta_{S_0}^* = (7,4,2,1,1)^T$, $\sigma^2$ was set to 0.1 to obey the asymptotic properties of the Lasso. Zhao et al. (2006) first draw the covariance matrix $\Sigma$ from $Wishart(p, I_p)$ and then generate design matrix $X$ from $N(0,\Sigma)$.

Zhao et al. (2006) showed that the irrepresentability condition may not hold for such a design matrix. In fact, in our simulation studies the $\eta_\infty$'s for the 100 simulated designs were between $[-1.02, 0.36]$. We expect the Lasso to perform well when $\eta_\infty>0$ and poorly when $\eta_\infty<0$. We generate $n = 100$ design matrices and for each design, 100 simulations were conducted by generating the noise vector from $N(0, \sigma^2 I)$. 

```{r, echo = T}
no_cores <- detectCores()-1
RNGkind("L'Ecuyer-CMRG")
cl <- makeCluster(no_cores)
registerDoParallel(cl)
clusterSetRNGStream(cl, 13)

niter = 100

# Uncomment this if you want a progress bar
# pb <- winProgressBar(title="Progress bar", label="0% done", min=0, max=100, initial=0)

n=100;p=32
p1 = 5
beta <- c(c(7,4,2,1,1),rep(0,p-p1))
index <- c(rep(1,p1),rep(0,p-p1))

res<- foreach(i = 1:niter, .packages = c("bayesm","horseshoe","glmnet"))%dopar%{
  
    sigma <- rwishart(p,diag(p))
    
    x=matrix(rnorm(n*p),n,p) # generate X matrices
    R <- chol(sigma$W)
    x <- x%*%R
    fx <- x%*% beta
    A <- t(x[,-(1:p1)])%*%x[,(1:p1)]%*%solve((t(x[,(1:p1)])%*%x[,(1:p1)]))%*%rep(1,p1)
    n_inf <- 1-norm(A,'i')
    reps = 50
    mp_hs = rep(0,reps)
    mp_lasso = rep(0, reps)

    for (j in 1:reps)
    { 
      eps=sqrt(0.1)*rnorm(n)
      y=drop(fx+eps)
      data = data.frame(y,x)
      ## Using horseshoe
      res <- horseshoe(y, x, method.tau = "truncatedCauchy",
                       method.sigma = "Jeffreys", burn = 1000, nmc = 1000)
      hs_ind <- HS.var.select(res, y, method = "intervals")
      mp_hs[j] <- sum((hs_ind!=index))
      
      lasso.mod = glmnet(x,y,alpha = 1)
      cv.out = cv.glmnet(x,y,alpha = 1, nfolds=10)
      bestlam = cv.out$lambda.min
      lasso.coef = predict(lasso.mod,type ="coefficients",s=bestlam)
      lasso_cv_est = lasso.coef[-1]
      lasso_ind <- ((lasso_cv_est!=0))
      mp_lasso[j] <- sum((lasso_ind!=index))
    }
    prop_true_hs <- sum((mp_hs==0))/reps
    prop_true_lasso <- sum((mp_lasso==0))/reps
    
    # info <- sprintf("%d%% done", round((i/niter)*niter))
    # setWinProgressBar(pb, i/(niter)*niter, label=info)
    
    cbind(n_inf,prop_true_hs,prop_true_lasso)
}

# close(pb)
stopCluster(cl)
registerDoSEQ()
```

Figure below shows the percentage of correctly selected model as a function of the irrepresentable condition number, $\eta_\infty$ for Lasso, the Robust prior, and the Horseshoe prior. 

As expected, Lasso's variable selection performance is crucially dependent on the irrepresentability condition but the Horseshoe prior almost always recovers the true sparse $\beta$ vector irrespective of $\eta_\infty$.

```{R, echo = F, fig.align = 'center'}
reps = 50

prop_true <- matrix(unlist(res), nrow = reps, ncol = 3, byrow = TRUE)
## write.table(prop_true, "prop_true_100_100_parallel.csv", sep=",")

eta.data <- rbind(data.frame(prop = prop_true[,2], type = "Horseshoe"),
                  data.frame(prop = prop_true[,3], type = "Lasso"))
eta.data <- cbind(eta.data, eta = prop_true[,1])

library(ggplot2)
eta.plot = ggplot(eta.data, aes(x=eta, y = prop)) + ylim(0,1)+
  geom_point(size=1) + facet_grid(type ~ ., scale="free")+ylab("Percentage of Correct Selection")+
  xlab(expression(eta[infty]))+theme_bw()

eta.plot <- eta.plot+theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"))
eta.plot <- eta.plot + theme(axis.title.y = element_text(size = rel(1.2), angle = 90))+
  theme(axis.title.x = element_text(size = rel(1.2)))

eta.plot<- eta.plot+ theme(axis.text = element_text(size = rel(1.2)))
eta.plot <- eta.plot+theme(strip.text.x = element_text(size=13.5, face="bold"),
                           strip.text.y = element_text(size=13.5, face="bold"))+theme(legend.position="none")
print(eta.plot)
```
