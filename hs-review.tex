\documentclass[11pt]{article}
%\documentclass[aos,preprint]{imsart} %IMS
%\RequirePackage[OT1]{fontenc} %IMS

\usepackage[slantedGreek]{mathpazo}
%\usepackage[pdftex]{graphicx}
\usepackage{amsfonts}
\usepackage{setspace}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{lscape}
%\usepackage{caption}
\usepackage{subcaption}
\setcounter{MaxMatrixCols}{30}
%\usepackage{eps2pdf}
\usepackage{suffix}
\usepackage{color}
\usepackage{bigstrut}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{epstopdf}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{nicefrac}


\usepackage[normalem]{ulem}

%\setlength{\textheight}{9in}
%\setlength{\textwidth}{6.5in}
%\setlength{\topmargin}{-36pt}
%\setlength{\oddsidemargin}{0pt}
%\setlength{\evensidemargin}{0pt}

% this order is important
%\usepackage{hyperref}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref} %IMS
%\usepackage[author-year]{amsrefs}

\usepackage[sort,comma]{natbib}

%\arxiv{1502.00560}
\renewcommand{\baselinestretch}{1.5}
\allowdisplaybreaks

\include{math-commands}
\graphicspath{{../../figures/}{../figures/}{./figures/}{./figs}{./}}

\title{Lasso Meets Horseshoe}

\author{Anindya Bhadra  \footnote{{\em Address:} 250 N. University St., West Lafayette, IN 47907, email: bhadra@purdue.edu.} \\Purdue University
\and Jyotishka Datta  \footnote{{\em Address:} SCEN 309, 1 University of Arkansas, Fayetteville, AR, 72701, email: jd033@uark.edu.}\\ University of Arkansas\\
\and Nicholas G. Polson \footnote{{\em Address:} 5807 S. Woodlawn Ave., Chicago, IL 60637, email: ngp@chicagobooth.edu.}  \ \ and Brandon T. Willard \footnote{{\em Address:} 5807 S. Woodlawn Ave., Chicago, IL 60637, email: bwillard@uchicago.edu.} \\The University of Chicago Booth School of
Business}
\date{\today}
\singlespacing
\begin{document}
\maketitle
%\baselineskip=15pt
\onehalfspacing


\begin{abstract}
\baselineskip=15pt
\noindent %We review Bayesian and classical approaches to regularization problems. 
The goal of the paper is to survey the major advances for Lasso and Horseshoe, which are sparse signal recovery regularisation methodologies. Lasso and its variants are a gold standard for selecting the best subset of predictors while Horseshoe is a state-of-the-art Bayesian procedure. Lasso has the advantage of being a scalable convex optimization method whilst the Horseshoe penalty is non-convex and the estimator is the posterior mean that minimizes Bayes risk under quadratic loss.  We provide a novel view from three aspects, (i) theoretical optimality, (ii) efficiency and scalability of computation and (iii) methodological development and performance in high dimensional inference for the Gaussian sparse model and beyond. 


{\bf Keywords:} Sparsity; Regression; lasso; global-local priors; horseshoe; horseshoe+; regularization; Hyper-parameter tuning. 
\end{abstract}

\section{Introduction}

%\textbf{Issues}
%\begin{itemize}
	%\item Minimaxity 
	%\item Sparse signal recovery: nearly black objects. 
	%\item Concentration rates.
	%\item Admissibility (Minimax not necessarily admissible : James--Stein / Ridge). 
	%\item Super-efficiency? (Hodges-Lehmann). 
	%\item Prediction 
	%\item Hyper-parameter / regularisation path (sensitivity analysis)
	%\item Model selection 
%\end{itemize}

%\subsection{Sparsity}

%\textcolor[rgb]{1,0.41,0.13}{Define typical sparse problem. Similarity with model subset selection (AIC, BIC). Sparsity/ Model Selection / ill-posed regularization problem.}

%The problem of sparse parameter estimation in high-dimensional models has featured prominently in modern statistics literature for at least the two previous decades. While the general area is too large to cover in a single review article, and entire books have been written on the subject \citep[see, e.g., ][]{hastie09}, the goal of this review article is more modest, if not more specific. We revisit at least two distinct approaches to sparse parameter estimation problems, primarily from a Bayesian point of view. The first of these assumes exact zeros in the model, necessitating the use of the so-called selection based approaches. The second operates on the view that while certain parameters might be small, an assumption of exact zeros is untenable, both philosophically and computationally. 

High-dimensional variable selection and sparse signal recovery has become a routine in many applications in Statistics and machine learning. This has led to a vast growing literature for both frequentist and Bayesian methodologies and computation to provide large scale inference estimators. Whilst the general area is too large to cover in a single review article, we revisit at least two popular approaches to sparse parameter estimation problems, the classical Lasso \citep{tibshirani96} and the Bayes horseshoe estimator \citep{carvalho2010horseshoe}. We compare and contrast three main areas: performance in high-dimensional data, theoretical optimality and computational efficiency. 

Formal definitions of sparsity and ultra-sparsity relates to the property of a few large signals among many zero or nearly zero noisy observations. A common goal in high dimensional inference problems is to identify the low-dimensional signals observed in white noise, i.e. to identify subset of hypotheses that show significant deviation from the null hypotheses. This encompasses three related, yet different areas: 
\ben
\item Estimation of the underlying sparse parameter vector. 
\item Multiple testing where the number of tests is much larger than the sample size, and 
\item Linear model where the number of covariates $p$ is much larger than the sample size $n$. 
\een

There is a rich variety of methodologies for high-dimensional inference based on regularisation that works by implicitly or explicitly penalizing models based on their dimensions. One of the most popular methods, Lasso (acronym for Least Absolute Shrinkage and Selection Operator) produces a sparse point estimate by constraining the $\ell_1$ norm of the coefficient vector. Lasso's widespread popularity is due to its computational efficiency based on the Least Angle Regression method due to \cite{efron_least_2004} as well as its ability to produce a sparse solution, with optimality properties for both estimation and variable selection. \cite{buhlmann2011statistics, james2013introduction, hastie2015statistical} provide excellent references for different aspcets of Lasso and its various modifications. \par 

The Bayesian alternatives to the sparse signal-recovery problem for high dimensional data can be broadly classified into two categories: discrete mixtures or ``two-groups" model or ``spike-and-slab" priors (vide \citet{johnstone2004needles,efron2010large,efron2008microarrays,bogdan2011asymptotic}) and shrinkage priors \cite{armagan2011generalized,armagan2013generalized,carvalho2009handling,carvalho2010horseshoe,griffin2005alternative,polson2010shrink,castillo2012needles}). The first approach is based on putting a point mass at zero and an absolutely continuous prior on the non-zero elements of the parameter vector. The second approach entails putting absolutely continuous shrinkage priors on the entire parameter vector, that shrink the entire coefficient towards zero. Both these approaches have their own advantages, and we discuss the trade-offs associated with choosing one over the other a little later.  It should also be noted that most of the penalization approaches can be interpreted in a Bayesian sense, by considering the mode of the posterior distribution under an appropriate shrinkage prior. 

%Ultra-sparse signal detection provides a challenge for developing statistical estimators. 
%\subsection{Nearly black normal means model: minimax rate in estimation and optimal rate in testing}

The rest of the paper is organized as follows: Section 2 provides some historical background for the normal means or the Gaussian compound decision problem and the normal regression problem. Section 3 provides the link between regularization and an optimization perspective viewed through a probabilistic Bayesian lens. Section 4 compares and contrasts the lasso and the horseshoe method and we provide discussion and directions for future work in Section 5. 

\section{Sparse Means, Regression and Variable Selection}

\paragraph{Sparse Normal Means:} As a starting point, suppose we observe data from the probability model $  (y_i \mid \theta_i)  \sim \Nor (\theta_i,1)$ for $i = 1, \ldots, n$. Our primary inferential goal is to estimate the vector of normal means $ \btheta = ( \theta_1, \ldots , \theta_n )$ and a secondary goal would be to simultaneously test if $\theta_i$'s are zero or not. We are interested in the sparse paradigm where a large proportion of the parameter vector contains zeros.  The ``ultra-sparse'' or ``nearly black'' vector case occurs when the parameter vector $\theta$ lies in the set $ l_0 [ p_n] \equiv \{ \theta : \# ( \theta_i \neq 0 ) \leq p_n \} $ with the upper bound on the number of non-zero parameter values $ p_n = o(n) $ as $ n \to \infty$. 

A natural Bayesian solution for inference under sparsity is the two-groups model that puts a non-zero probability spike at zero and a suitable prior on the non-zero $\theta_i$'s. The inference is then based on the posterior probabilities of non-zero $\theta_i$'s based on the discrete mixture model. The two-groups model possesses a number of frequentist and Bayesian optimality properties. \cite{johnstone2004needles} showed that a thresholding-based estimator for $\theta$ under the two-groups model with an empirical Bayes estimate for $\mu$ attains the minimax rate in $\ell_q$ norm for $q \in (0,2]$ for $\theta$ that are iether nearly black or belong to an $\ell_p$ ball of `small' radius. \cite{castillo2012needles} treated a full Bayes version of the problem and again found an estimate that is minimax in $\ell_q$ norm for mean vectors that are either nearly black or have bounded weak $\ell_p$ norm for $p \in (0,2]$. 

We now turn to the problem of variable selection in sparse regression. 

\paragraph{Sparse Linear Regression:} A related inferential problem is high dimensional linear regression with sparsity constraints on the parameter vector $\btheta$. We are interested in the linear regression model $\Y = \X \btheta + \bepsilon$, where $\X = [\X_1, \cdots, \X_p]$ is a $p \times n$ matrix of predictors and $\bepsilon \sim \Nor(\0, \I)$. Our focus is on the sparse situation where $p \gg n$ and ``most" of $\theta_i$'s are zero. Like the sparse normal means problem, our goal is to identify the non-zero entries of $\btheta$ as well as estimate it. There is a wide variety of methods based on the penalized likelihood approach that solves the following optimization problem:
\begin{align}
\min_{\btheta} \sum_{i=1}^{n} &  \left( y_i - \theta_0 - \sum_{j=1}^{p} \theta_j x_{i,j} \right)^2 + \text{pen}_{\lambda}(\btheta), \label{eq:penalize} \\
\text{where } & \text{pen}_{\lambda}(\btheta) = \sum_{j=1}^{p} p_{\lambda}(\theta_j) \text{ is a separabale penalty} \nonumber
\end{align}
 The most popular of these methods is the Lasso that uses an $\ell_1$ penalty, i.e. $p_{\lambda}(\theta_j) = -\lambda \abs{\theta_j}$, that simultaneously performs variable selection while maintaining estimation accuracy. Another notable variant is the best subset selection procedure corresponding to the $\ell_0$ penalty $p_{\lambda}(\theta_j) = -\lambda \1\{\theta_j \ne 0\}$. There has been a recent emphasis on non-concave separable penalties such as MCP \citep{zhang2010nearly} or SCAD \citep{fan2001variable}, that also act as a tool for variable selection and estimation. Penalized methods can also be treated as finding the posterior mode under a prior which relates to the penalty in \eqref{eq:penalize} via $p_{\lambda}(\btheta) = -\log(\pi(\btheta))$. We discuss the penalization methods from a Bayesian viewpoint in the next section. 

\paragraph{Variable Selection:} The problem of variable selection or model selection is intimately related to the sparse linear regression. It's well known that a sparse model renders interpretability, computational efficiency, and stability of inference. In fact, the ``bet on sparsity'' principle \citep{hastie09} dictates us to use methods that favours sparsity, since no method does well when the true is dense. Lasso's success has inspired many learning methods relying on convexity and sparsity that are built on this philosophy. 

There has also been a parallel surge of Bayesian methods for sparse regression problems with an underlying variable selection procedure. A typical hierarchical Bayesian method proceeds by selecting a model dimension $s$, selecting a random subset $S$ of dimension $\abs{S} = s$ and a prior $\pi_S$ on $\mathbb{R}^{S}$. The prior can be written as \cite{castillo2015bayesian}:
\beq
(S,\btheta) \mapsto \pi_p(\abs{S}) \frac{1}{\binom{p}{\abs{S}}} \pi_S(\btheta_S)\delta_{0}(\btheta_{S^c}) \label{eq:bayes-hier}
\eeq
Bayesian methods using similar approaches for sparse linear regression include \citep{george2000variable,George0000, mitchell88, ishwaran2005spike} and more recently \cite{rovckova2016spike}, who introduced the spike-and-slab Lasso prior. In \cite{rovckova2016spike}'s formulation, the hierarchical prior on the parameter and model spaces assumes the form:
\beq
\pi(\btheta \mid \gamma) = \prod_{i=1}^{p} [\gamma_i \pi_1(\theta_i) + (1-\gamma_i) \pi_0(\theta_i)], \quad \gamma \sim p(\cdot), \label{eq:ssl}
\eeq
where, $\bgamma$ indexes the $2^p$ possible models, and $\pi_0$, $\pi_1$ model the null and non-null $\theta_i$'s respectively using two Laplace priors with different scales. 

Despite the attractive theoretical properties outlined above, the discrete indicators in spike-and-slab models give rise to a combinatorial problem. While some posterior point estimates such as the posterior mean or quantiles might be easily computable for spike-and-slab \citep{castillo2012needles, castillo2015bayesian}, exploring the full posterior using Markov chain Monte Carlo (MCMC) is typically more challenging using point mass mixture priors. %A primary reason for this is exploring the posterior results in a combinatorial problem due to the discrete indicators and block updates of model parameters is also typically not feasible. Thus, one has to resort to some variation of a stochastic search algorithm in MCMC for model fitting, such as the stochastic search variable selection of \citet{george93, george97} or shotgun stochastic search of \citet{hans2007}.
\citet{rovckova2016spike} commented on the inefficiency of the stochastic search algorithms for exploring the posterior even for moderate dimensions and developed a deterministic alternative to quickly find the maximum a-posteriori model. We note that (i) increasing the efficiency in computation in the spike-and-slab model remains an active area of research \citep[see, e.g., ][]{rovckova2016spike} and (ii) some complicating factors in the spike-and-slab model, such as a lack of suitable block updates, have fairly easy solutions for their continuous global-local shrinkage counterparts, facilitating posterior exploration. 

The continuous one-group shrinkage prior has a different motto: instead of putting a prior on the model space to yield a sparse estimator, they model the posterior inclusion probabilities $P(\theta_i \ne 0 \mid y_i)$ directly, thus leading to fast computation. In a series of papers \citet{carvalho2009handling,polson2010shrink, carvalho2010horseshoe, polson2012half} introduced the `global-local' shrinkage priors. Global-local priors adjust to sparsity via global shrinkage, and identify signals by local shrinkage parameters. The global-local shrinkage idea has resulted in many different priors in the recent past, with a varying degree of theoretical and numerical performance. We provide a brief survey and comparison of these different priors and introduce a recently proposed horseshoe-like prior in Section \ref{sec:one-gp}.   

We would also like to point out that the estimators resulting from these one-group shrinkage priors are very different from the shrinkage estimator due to James-Stein, who proved that the maximum likelihood estimators for normal data are inadmissible beyond $\mathbb{R}^2$. James-Stein estimator only worries about the total squared error loss, without much concern for the individual estimates. In problems involving observations lying far away on the tails, this could lead to `over-shrinkage'. In reality, an ideal signal-recovery procedure should be robust to large signals.

%polson2010shrink

\section{Lasso and Horseshoe}
%Define estimators / compare shrinkage profiles. Asymptotic of finding zeroes. 
%Explain $\phi(\theta)$ for horseshoe etc. 
\subsection{Bayesian Regularization : A Useful Duality}

Regularization requires the researcher to specify a measure of fit, denoted by $l(\theta)$ and a penalty function, denoted by $ \phi(\theta)$. Probabilistically,  $l(\theta)$ and $\text{pen}_{\lambda}(\theta)$ correspond to the negative logarithms of the likelihood and prior distribution, respectively.  Regularization leads to an optimization problem of the form 
\begin{equation}
\label{eqn:reg}
\begin{aligned}
& \underset{\theta \in \Re^d}{\text{minimise}}
& & l(\theta) + \text{pen}_{\lambda}(\theta) \; . 
\end{aligned}
\end{equation}
The probabilistic approach leads to a Bayesian hierarchical model
\beq
p(y \mid \theta) \propto \exp\{-l(\theta)\} \; , \quad p_{\lambda}(\theta) \propto \exp\{ -\text{pen}_{\lambda}(\theta) \}. \label{eq:pen}
\eeq
The solution to the minimisation problem estimated by regularisation (\ref{eqn:reg}) corresponds to the posterior mode, $ \hat{\theta} = {\rm arg \; max}_\theta \; p( \theta|y) $, where $ p(\theta|y)$ denotes the posterior distribution \citep{polson2015mixtures}. The properties of the penalty are then induced by those of the prior. For example, regression with a least squares log-likelihood subject to a penalty such as an $\ell_2$-norm (ridge) \citep{hoerl70} Gaussian probability model or $\ell_1$-norm (lasso) \citep{tibshirani96} double exponential probability model. 

One can think of the lasso and related $L^1$ penalty based methods as trying to perform selection, while ridge and related $\ell_2$ based methods perform shrinkage. Selection-based methods such as the lasso are unstable in many situations, e.g., in presence of multicollinearity in the design. Shrinkage often wins in terms of predictive performance. But shrinkage methods do not give exact zeros, which is preferred over dichotomous models by some practitioners \citep{stephens2009bayesian}. Thus, both selection and shrinkage have their advantages and disadvantages. 

\subsection{Lasso Penalty and Prior}\label{sec:lasso}

The Lasso based estimate of $\btheta$ is the value of $\btheta$ that maximizes the $\ell_1$ penalized log-likelihood, or equivalently, the posterior mode under a component-wise Laplace prior, as given below: 
\begin{equation}
(\text{Penalty}): \; \text{pen}_{\lambda}(\btheta) = \lambda \sum_{j = 1}^{p} \abs{\theta_j} \; \equiv \; \pi_{\lambda}(\btheta) = \exp( - \lambda \sum_{j = 1}^{p} \abs{\theta_j}) \; (\text{Prior})
\end{equation}

Since the posterior mode is the same as the classical Lasso-based estimate, the posterior mode possesses the optimal properties of Lasso proved by \cite{buhlmann2011statistics}. For example, the Oracle inequality in \citet[Eq. (2.8), Th. (6.1)]{buhlmann2011statistics} states that up to $O(\log(p))$ and a compatibility constant $\phi_0^2$, the mean squared prediction error is of the same order as if one knew active set $S_0 = \{j : \theta_j^0 \neq 0 \}$.)  Lasso also exhibits other desirable properties such as computational tractability, consistency of point estimates of $\btheta$ for suitably tuned $\lambda$, and optimality results on variable selection. 

As we discussed before, the posterior mode of $\btheta$ under the double exponential prior will have all the above optimal properties of the Lasso estimate of $\btheta$. Unfortunately, the same is not expected to hold for the posterior mean, which is the Bayes estimate under squared error loss. In fact, \citet{castillo2015bayesian} argue that the Lasso is essentially non-Bayesian, in that the ``\textsl{full posterior distribution is useless for uncertainty quantification, the central idea of Bayesian inference}''. \citet{castillo2015bayesian} provide theoretical result that the full Lasso posterior does not contract at the same speed as the posterior mode. 

The deficiency of the double exponential prior for shrinking small observations while maintaining robustness to the large observations is noted by various people, including \cite{polson2010shrink,datta2013asymptotic}, the key property of global-local priors. 

%the naive estimate $ \hat{S}(\lambda) = \{ j; \hat{\btheta}_j^{Lasso}(\lambda) \neq 0, j = 1, \ldots, p_n \}$ satisfies $P( \hat{S}(\lambda) = S_0) \to 1$ as $n \to \infty$ under a certain condition called ``the neighbourhood stability condition" on the design matrix and ``beta-min" condition on the coeffcient vector \footnote{The ``beta-min" condition is requiring that the non-zero coefficients are sufficinetly large, i.e. the non-zero $\beta_j^0$s satisfy $ \inf_{j \in \S_0^c} |\beta_j^0| >> \sqrt{s_0 \log(p)/n}$. For further discussion on this, please refer to Ch. 7 in \cite{buhlmann2011statistics}.}(\textit{vide} Theorems 1,2 in \cite{meinshausen2006high} and Section 2.6.1  in \cite{buhlmann2011statistics}). The neighbourhood stability condition is equivalent to the ``irrepresentable" condition of \cite{zhao2006model,zou2006adaptive}. 

%We describe the ``irrepresentable" condition below:\par
%Suppose, the sample covariance matrix is denoted by $\hat{\Sigma} = n\X^T\X$ and the active-set $S_0 = \{ j : \beta_j \neq 0\}$ consists of first $s_0$ elements of $\beta$. One can partition the $\hat{\Sigma}$ matrix as 
%\begin{equation}
%\hat{\Sigma} = \left(\begin{array}{cc}    
%\hat{\Sigma}_{s_0,s_0} & \hat{\Sigma}_{s_0,p-s_0}\\
%\hat{\Sigma}_{p-s_0,s_0} & \hat{\Sigma}_{p-s_0,p-s_0}
%\end{array} \right) 
%\end{equation}
%where $\hat{\Sigma}_{s_0,s_0}$ is a $s_0\times s_0$ matrix corresponding to the active variables and so on. The irrepresentable condition for variable selection consistency of Lasso is:
%\begin{equation}
%\vectornorm{ \hat{\Sigma}_{p-s_0,s_0} \hat{\Sigma}_{s_0,s_0}^{-1} sign(\beta_{S_0}) }_{\infty} \leq \theta \quad \mbox{ for some } 0 < \theta < 1
%\end{equation}
%This condition is sufficient and almost necessary in the sense that the necessary condition is only slightly weaker than the sufficient condition. The necssary condition requires `$\leq 1$', while the sufficient condition involves $\leq \theta$ for some $0 < \theta < 1$. The irrepresentable condition fails to hold if the design matrix is too ill-posed, i.e. has multi-collinearity.\cite{buhlmann2011statistics} warn the readers that the irrepresentable condition may fail even though the design matrix is not ill-posed and it might restrict what can be done in high-dimensional problems. \cite{zhao2006model} provide numerical example to show the effect of the irrepresentable condition on the variable selection performance of Lasso. They showed that the probability of selecting the true sparse model is an increasing function of the irrepresentability condition number, defined as 
%\begin{equation}
%n_{\infty} = 1 - \vectornorm{ \hat{\Sigma}_{p-s_0,s_0} \hat{\Sigma}_{s_0,s_0}^{-1} sign(\beta_{S_0}) }_{\infty}
%\end{equation}
%In particular, the probability of Lasso selecting the true model is almost 1 when $n_{\infty} > 0.2$ and it is almost zero when $n_{\infty} < -0.3$. Later, in Section \ref{jd:sec:simu} we examine how badly the irrepresentability condition affects the variable screening properties of the Robust prior and the Horseshoe prior based model selection.

For correlated predictors, \cite{zou2005regularization} proposed a family of convex penalty called `elastic net', which is a hybrid between Lasso and Ridge. The penalty term is $\sum_{j=1}^{p} \lambda p_{\alpha}(\theta_j)$, where 
\[
p_{\alpha}(\theta_j) = \half (1-\alpha)\theta_j^2 + \alpha \abs{\theta_j}, \quad j = 1, \ldots, p. 
\]
Both Lasso and Elastic net facilitate efficient Bayesian computation via a global-local scale mixture representation \cite{bhadra2016global}. The Lasso penalty arises as a Laplace global-local mixture \citep{andrews1974scale}, while the elastic-net regression can be recast as a global-local mixture with a mixing density belonging to the orthant-normal family of distributions \citep{hans2011elastic}.  The orthant-normal prior on $\theta_i$, given hyper-parameters $\lambda_1$ and $\lambda_2$, 
has a density function with the following form:
\begin{equation}
  p(\theta_i \mid \lambda_1, \lambda_2)  = 
  \begin{cases} 
   \phi(\theta_i \mid \frac{\lambda_1}{2\lambda_2}, \frac{\sigma^2}{\lambda_2}) / 2\Phi\left(-\frac{\lambda_1}{2\sigma \lambda_2^{1/2} }\right), & \quad \theta_i < 0, \\
   \phi(\theta_i \mid \frac{-\lambda_1}{2\lambda_2}, \frac{\sigma^2}{\lambda_2}) / 2\Phi\left(-\frac{\lambda_1}{2\sigma \lambda_2^{1/2} }\right), & \quad \theta_i \geq 0. \end{cases} 
  \label{eq:hans}
\end{equation}
 

\subsection{Horseshoe Penalty and Prior}\label{sec:one-gp}

%Exponential integral function and bound $ \log (1 + 1/ \theta^2 ) $. Horseshoe \citep{carvalho2009handling, carvalho2010horseshoe, polson2010shrink, polsonscott2012} and horseshoe+ \citep{bhadra2015horseshoe+} proceed by shrinkage, so the combinatorial problem does not arise.

In a series of remarkable papers, \citet*{carvalho2009handling, carvalho2010horseshoe, polson2010shrink, polson2012half} introduced a continuous ``one-group" shrinkage rule based on what they call the horseshoe prior for multiple testing and model selection. The horseshoe prior for $\theta_i$, given a global shrinkage parameter $\tau$ is given by: 
\begin{align}
Y_i & \sim \Nor(\theta_i , \sigma^2), \theta_i \sim \Nor(0, \lambda_i^2 \tau^2) \nonumber \\
\lambda_i ^2 & \sim \operatorname{C}^{+}(0,1), \quad i = 1, \ldots, n. \label{eq:hs}
\end{align}

As discussed before, the horseshoe prior operates under a different philosophy: that of modeling the inclusion probability directly rather than using a discrete mixture to model sparsity. To see this, note that the posterior mean under the horseshoe prior can be written as a linear function of the observation:
\beq
\E(\theta_i \mid y_i) = (1- \E(\kappa_i \mid y_i))y_i \text{ where } \kappa_i = 1/(1+\lambda_i^2 \tau^2)
\eeq
The name `Horseshoe' is attributed to the shape of the ${\rm Be} (\half, \half)$ prior density of the shrinkage weight, $\kappa_i$, for each observation. A comparison with the posterior mean obtained under the two-groups model reveals that the shrinkage weights perform the same job as the posterior inclusion probability $P(\theta_i \ne 0 \mid y_i)$ for recovering a sparse signal. Since the shrinkage coefficients are not formal Bayesian posterior quantities, we refer to them as `pseudo posterior inclusion probabilities'. \citet{carvalho2010horseshoe} provided strong numerical evidence that this ``one-group" shrinkage rule approximately behaves like the answers from a two-groups model under sparsity and attains super-efficiency in reconstructing the true density. Although, the main goal of a shrinkage prior is estimation, this interpretation of shrinkage weights as inclusion probabilities led \citet{carvalho2010horseshoe} to propose a multiple testing rule by using a threshold on $1-\hat{\kappa}_i$ values. \citet{datta2013asymptotic} investigated the theoretical optimality of such a decision rule under a $0$-$1$ additive loss and showed that the horseshoe multiple testing rule attains the Bayes oracle up to a multiplicative constant. 

We focus here on the behaviour of the posterior distribution under the horseshoe prior. Although the prior density under the horseshoe prior doesn't admit a closed form, we can write the horesehoe posterior mean by using the Tweedies' formula $\E(\theta \mid y) = y + \dd{\ln m(y)}{y}$ as: 
\beq
\E(\theta_i \mid y_i, \tau) = y_i \left( 1 - \frac{2\Phi_1(\half, 1, \frac{5}{2},\frac{y_i^2}{2\sigma^2}, 1-\frac{1}{\tau^2})}{3\Phi_1(\half, 1, \frac{3}{2},\frac{y_i^2}{2\sigma^2}, 1-\frac{1}{\tau^2})} \right)
\eeq

where $\Phi_1$ is the degenerate hypergeometric function of two variables. This enables one to rapidly calculate the posterior mean estimator under the horseshoe prior via a `plug-in' approach with estimated values of the hyperparameter $\tau$. \citet{van2014horseshoe} showed that the empirical Bayes posterior mean estimator enjoys  a `near-minimax' rate of estimation if the global shrinkage parameter $\tau$ is chosen suitably. We discuss the statistical properties of horseshoe posterior mean estimator and the induced decision rule in more details in \S \ref{sec:stat-prop}. 

%
The horseshoe prior is a member of a wider class of global-local scale mixtures of normals that admit following hierarchical form \citep{polson2010shrink}: 
\begin{gather*}
\y \sim \Nor(\X \btheta, \sigma^2 \I) ;\; \theta_i \sim \Nor(0, \lambda_i^2 \tau^2) \\
\lambda_i^2 \sim \pi(\lambda_i^2) ; \; (\tau,\sigma^2) \sim  \pi(\tau^2,\sigma^2), i = 1, \ldots, n. 
\end{gather*}
These priors are collectively called the ``global-local" shrinkage priors after \cite{polson2010shrink} as they recover signals by a local shrinkage parameter and adapt to sparsity by a global shrinkage parameter. Some of the popular shrinkage priors include the Generalized Double Pareto (GDP) \citep{armagan2013generalized}, the three-parameter Beta \citep{armagan2011generalized}, and the more recent horseshoe+ \citep{bhadra2015horseshoe+} and the Dirichlet-Laplace \citep{bhattacharya2014dirichlet} priors. A natural question is \textit{how do we compare between these priors?} It is known due to several authors \citep[e.g.]{polson2010shrink,bhadra2015default,van2015conditions} that the key features of a global-local shrinkage prior is a peak at origin and heavy tails. Below we list a few popular global-local shrinkage priors along with their behaviour near origin and the tails. A detailed list of shrinkage priors proposed in the recent past is deferred to \S \ref{sec:app-ext}.

\begin{table}%
\centering
\begin{tabular}{| c | c |c |}
\hline
Prior & Origin Behavior & Tails \\
\hline 
Horseshoe & $-\log(\abs{\theta})$ & $\abs{\theta}^{-2}$ \\
Horseshoe+ & $-\log(\abs{\theta})$ & $\abs{\theta}^{-1}$ \\
Horseshoe-like & $-\abs{\theta}^{1-\epsilon}\log(\abs{\theta})$ & $\abs{\theta}^{1-\epsilon}$ $\epsilon \ge 0$\\
GDP & Bounded at origin & $\abs{\theta}^{-(\alpha + 1)}, \alpha \ge 0$ \\
$DL_{a}$ ($DL_{\frac{1}{n}}$) & $\abs{\theta}^{a-1}$ ($\abs{\theta}^{\frac{1}{n}-1}$) & $\exp(-b\abs{\theta})$ \\
\hline
\end{tabular}
\caption{Different Priors: Behaviour near origin and tails}
\label{tab:priors}
\end{table}

\begin{figure}[ht!]
% \centering 
  \begin{subfigure}{0.45\linewidth}
	\includegraphics[height=2.5in,width=\textwidth]{densities_zero_new}%
	\caption{\footnotesize{Marginal prior densities near the origin. The legends denote the horseshoe+ (HSPlus), horseshoe (HS), Dirichlet-Laplace (DL), generalized double Pareto (GDP), Cauchy and Laplace priors.}}
	\label{fig:zero}
	\end{subfigure}
	\hspace{0.1in}
  \begin{subfigure}{0.45\linewidth}
	\includegraphics[height=2.5in,width=\textwidth]{densities_tails_new}
 	 \caption{\footnotesize{Marginal prior densities in the tail regions. The legends denote the horseshoe+ (HSPlus), horseshoe (HS), Dirichlet-Laplace (DL), generalized double Pareto (GDP), Cauchy and Laplace priors.}}
  \label{fig:tails}
		\end{subfigure}
\end{figure}

One way to judge a prior is by the penalty it induces in a regularisation framework \eqref{eqn:reg}. For a prior $p(\theta)$, the induced penalty is given by $-\log(p(\theta))$ \eqref{eq:pen}. Although the horseshoe prior leads to optimal performance as a shrinkage prior, the induced penalty does not admit a closed form as the marginal prior is not analytically tractable. This poses a hindrance in learning via Expectation-Maximization or other similar algorithms. The generalized double Pareto prior of \citet{armagan2011generalized} admits a closed form solution, but it does not have an infinite spike near zero needed for sparse recovery. Motivated by this, \citet{bhadra2017horseshoe} recently proposed the `horseshoe-like' prior by normalizing the tight bounds for the horseshoe prior. Thus, the horseshoe-like prior attains a unique status within its class: it has a closed form marginal prior for $\theta_i$, yet with a spike at origin and heavy tails and more importantly, admits a global-local scale mixture representation. The scale mixture form supports both a traditional MCMC sampling for uncertainty quantification in full Bayes inference and EM/MM or proximal learning when computational efficiency is the primary concern. Since the aim of designing a sparsity prior is achieving higher spike near zero while maintaing regularly varying tails, a useful strategy is to split the range of the prior into disjoint intervals: $[0,1)$ and $[1, \infty)$, and aim for higher spike in one and heavier tail in the other. This leads to a class of `horseshoe-like' priors with more flexibility in shape than any single shrinkage prior. We provide the form of the horseshoe-like prior and the general family and a key representation theorem, and direct the interested readers to \citet{bhadra2017horseshoe} for more details: 

\begin{description}
\item [Horseshoe-like prior] The horseshoe-like prior \citet{bhadra2017horseshoe} has the following marginal prior density for $\theta_i$: 
\beq
\tilde p_{\tilde{HS}} (\theta_i \mid \tau^2) = \frac{1}{2 \pi{\tau}}\log \left ( 1 + \frac{\tau^2}{\theta_i^2} \right ), \quad  \; \theta_i  \in \mathbb{R},\; \tau > 0. \label{eq:hslike}
\eeq
\item [Horseshoe-like family] The general family of horseshoe-like prior can be constructed as a density split into disjoint intervals as follows:
\begin{align}
p_{hs}(\theta_i \mid \tau^2)&  \propto \begin{cases} 
      \frac{1}{\theta_i^{1-\epsilon}}\log \left( 1 + \frac{\tau^2}{\theta_i^2} \right) & \text{ if } {\abs{\theta_i} < 1} \\
      \theta_i^{1-\epsilon} \log \left( 1 + \frac{\tau^2}{\theta_i^2} \right) & \text{ if }{\abs{\theta_i} \ge 1}, \\
      \end{cases} 
\; \epsilon \ge 0,\tau > 0. \label{eq:split} 
\end{align}
\item [Normal scale mixture] The horseshoe-like prior \eqref{eq:hslike} is a Gaussian scale mixture with a Slash normal density, which is in turn a Normal scale mixture of ${\rm Pareto}(1/2)$ density, yielding the following representation theorem: 
\begin{theorem}\label{th:hslike}
The horseshoe-like prior in \eqref{eq:hslike} has the following global-local scale mixture representation:
\begin{align}
(\theta_i \mid t_i, \tau) \sim \Nor\left(0, \frac{\tau^2}{t_i^2} \right), (t_i \mid s_i)  \sim \Nor\left(0, s_i \right), \; & s_i \sim \mathrm{Pareto}\left( \frac{1}{2} \right), \label{eq:pareto}\\ 
& t_i \in \mathbb{R}, \; \tau \ge 0. \nonumber
\end{align}
\end{theorem}
\end{description}



%\subsection{Other global-local examples}
%Three parameter beta \citep{armagan2011generalized}; generalized double Pareto \citep{armagan2013generalized}; generalized shrinkage prior \citep{denison12}; Dirichlet-Laplace \citep{bhattacharya2014dirichlet} ; normal-exponential-gamma \citep{griffin2005alternative}; R2-D2 \citep{zhang2016high}. 
%
   %\begin{figure}[!t]
   	%\centering
   	%\includegraphics[width=12.5cm, height=5cm]{HorseshoePrior.pdf}
   	%\caption{Horseshoe prior near zero and at the tails}
   	%\label{fig:hs}
   %\end{figure}
   %
      %\begin{figure}[!h]
   	%\centering
   	%\includegraphics[width=12.5cm, height=5cm]{fourkappas.pdf}
   	%\caption{Plot of kappas}
   	%\label{fig:kappa}
   %\end{figure}
   %
         %\begin{figure}[!h]
   	%\centering
   	%\includegraphics[width=12.5cm, height=5cm]{horseshoe.png}
   	%\caption{Regularization}
   	%\label{fig:reg}
   %\end{figure}

\subsection{Shrinkage Profiles and Sparsity}

The better performance of horseshoe and horseshoe+ priors compared to double exponential or normal can also be explained by their shrinkage profile. Consider the marginal likelihood for the normal means model: $p(y_i \mid \kappa_i, \tau) = \kappa_i^{1/2} \exp \left(-\kappa_i y_i^2/2 \right)$. The posterior density of $\kappa_i$ identifies signals and noises by letting $\kappa_i \to 0$ and $\kappa_i \to 1$ respectively. Since the marginal likelihood puts no probability density on $\kappa_i = 0$, it does not help identifying the signals. Intuitively, any prior that drives the probability to either extremeities should be a good candidate for sparse signal reconstruction. The horseshoe prior does exactly that: it cancels the $\kappa_i^{1/2}$ term and replaces it with a $(1-\kappa_i)^{-1/2}$ to enable $\kappa_i \to 1$ in the posterior. The horseshoe+ prior takes this philosophy one step further, by creating a $U$-shaped Jacobian for transformation from $\lambda$ to $\kappa$-scale. The double-exponential on the other hand, yields a prior that decays at both ends with a mode near $\kappa_i = 1/4$ - thus leading to a posterior that is neither good at adjusting to sparsity, nor recovering large signals. 

%\begin{center}
%\begin{wrapfigure}{r}{0.5\textwidth}
\begin{figure}[!ht]
%\vspace{-0.2in}
\centering
\includegraphics[width=0.48\textwidth]{prior_diff_kappa}
\caption{\footnotesize{Shrinkage profile for Horseshoe, Horseshoe+, and Laplace prior.}}
\label{fig:priorkappa}
\end{figure}
%\end{wrapfigure}
%\end{center}
%\vspace{-0.9cm}

\begin{table}[!ht]
\centering
%\def~{\hphantom{0}}
\caption{Priors for $\lambda_i$ and $\kappa_i$ for a few popular shrinkage rules}
%{\footnotesize
\begin{tabular}{ccc}
\hline
Prior for $\theta_i$ & Prior for $\lambda_i$ & Prior for $\kappa_i$ \\ 
\hline \\
%GDP & $\frac{\sqrt{2}}{(\lambda_i^2)} \int_{0}^{\infty} \exp \bigg(\sqrt{\frac{2u}{\lambda_i^2}} - u \bigg) \sqrt{u} \mathrm{d}u$  & $\frac{1}{2(1-\kappa)^2} \left[ \frac{\sqrt{\pi} \exp \left\{ \frac{\kappa}{2(1-\kappa)} \right\} Erfc \left\{ \sqrt{\frac{\kappa}{2(1-\kappa)}} \right\}  }{\sqrt{2\kappa(1-\kappa)}} - 1 \right]$ \\[20pt]
Horseshoe & $2/ \left\{ \pi \tau (1 + (\lambda_i/\tau)^2 )\right\}$  & $\frac{\tau}{\sqrt{\kappa_i (1-\kappa_i )}} \frac{1}{(1+\kappa_i (\tau^2 -1 ) )}$ \\[10pt]
Horseshoe+ & $\frac{4\log \lambda_i/\tau}{\left\{{\pi^2 \tau}(\lambda_i/\tau)^2 -1)\right\}}$ &  $\frac{\tau}{\sqrt{\kappa_i (1-\kappa_i )}}\frac{\log \left \{ ( 1 - \kappa_i ) / \kappa_i \tau^2 \right \}}{ (1-\kappa_i (\tau^2 +1 ))}$ \\[10pt]
Double Exponential & $\lambda_i \exp (-\lambda_i^2/2)$ & $\kappa_i^{-2} \exp{-\frac{1}{2\kappa_i}}$ \\
\hline 
\end{tabular}
%}
\end{table}

%\section{Bayesian Regularization : A Useful Duality }
%\subsection{Feature Extraction via Non-convex Penalty:} For the sparse normal means model where $\#(\theta_i \ne 0)\le p_n$ and $p_n = o(n)$ as $n \to \infty$, non-convex regularization problems arise from a need to correctly identify the zero components in $\theta$, also known as subset selection. The  $\ell_0$ penalty, defined as $||\theta||_0 =\sum_{i=1}^{n} 1(|\theta_i|>0)$, is ideal for this task, and the more commonly used lasso or convex $\ell_1$ penalty, $||\theta||_1 = \sum_{i=1}^{n} |\theta_i|$, tends to select a denser model \citep{mazumder2012}. Unfortunately, naively using the $\ell_0$ penalty requires a combinatorial evaluation of all $2^n$ models, which is NP-hard \citep{natarajan95}. Penalties of the form $\ell_\gamma$ for $\gamma \ge 1$ give rise to convex problems and efficient solvers are available. It remains a challenge to fit models with $\ell_\gamma$ penalties for $\gamma\in (0,1)$. While this does not necessarily a present combinatorial problem, the regularization problem is non-convex. Thus, the general purpose tools for convex optimization do not apply, nor is a unique solution guaranteed \citep[see, e.g.,][Chapter 1]{boyd2004convex}. 
%Non-convex penalties include the smoothly clipped absolute deviation or SCAD \citep{fan2001variable} and the minimax concave penalty or MCP \citep{zhang2010nearly}. Recent computational advances in fitting models with non-convex penalties include \citet{breheny2011coordinate} and \citet{mazumder2012}. Both works use coordinate descent approaches to fit SCAD and MCP and provide conditions for convergence.  Alternatively, an overview of proximal algorithms for non-convex optimization can be found in \citet{polson2015proximal}. Recent works have also demonstrated the equivalence between fitting a model with MCP penalty and evaluating the posterior mode in a Bayesian hierarchical model under a suitable prior \citep{strawderman2013hierarchical, schifano2010majorization}. Following along these lines, we show that evaluating the posterior mode under a suitable approximation to the horseshoe prior of \citet{carvalho2009handling, carvalho2010horseshoe} solves a non-convex optimization problem with desirable theoretical properties and derive a fast computational algorithm. %Using a latent variable representation in our probabilistic framework, we develop an expectation-maximization (E-M) algorithm \citep{dempster1977} that quickly reaches a local optimum, usually in a fraction of time required by the non-convex solvers.
%\cite{bhadra2017horseshoe} provides results on feature selection properties of horseshoe-like penalties. 

\section{Statistical Properties}\label{sec:stat-prop}

\paragraph{History of Shrinkage Estimation: Inadmissibility of MLE } The story of shrinkage estimation harks back to Charles Stein's proof in 1955 that the maximum likelihood estimators for normal data are inadmissible beyond $\mathbb{R}^2$. The James-Stein estimator is $\hat{\theta}^{JS} = (1 - \nicefrac{(m-2)}{\vectornorm{\Y}^2})Y$ with posterior mean $\hat{\theta}_{\mathrm{Bayes}} = (\nicefrac{\tau^2}{\tau^2+1}) Y$, which corresponds to the Bayes risk of $m(\nicefrac{\tau^2}{\tau^2+1})$. %Without any prior guess about $\tau^2$, one could use an Empirical Bayes estimate and the resulting James-Stein estimator is $\hat{\theta}^{JS} = (1 - \nicefrac{m-2}{\vectornorm{\Y}^2})Y$. 
James and Stein proved that this estimator dominates the MLE in terms of the expected total squarred error for every choice of $\btheta$, i.e. it outperforms the MLE no matter what the true $\btheta$ is. To motivate the need for developing new prior distributions, consider the classic James--Stein ``global'' shrinkage rule, $\estJs(y)$. This estimator uniformly dominates the traditional sample mean estimator, $\hat{\theta}$. For all values of the true parameter $\theta$ and for $n>2$, we have the classical mean squared error (MSE) risk bound:
\[
R(\estJs, \btheta) \defeq \E_{y \mid \btheta} {\Vert \estJs(\y) - \btheta \Vert}^2 < n = \E_{\y \mid \btheta} {\Vert \y - \btheta \Vert}^2, \quad \forall \btheta \in \mathbb{R}^n, \; n \ge 3.
\]
%However, for a sparse signal, $\estJs(y)$ performs poorly. Suppose that the true parameter $\theta$ is an `$r$-spike'' with $r$ coordinates of magnitude $ \sqrt{n/r}$ and the rest set at zero, giving $ {\Vert \theta \Vert}^2 =n$. Then \cite{johnstone2004needles} showed that the classical risk satisfies $ R \left ( \estJs , \theta \right ) \geq n/2 $ whereas simple thresholding at \sqrt{2 \log n}$ performs with risk $\sqrt{\log n}$. 

However, for the sparse signal problem, the standard James-Stein shrinkage rule $ \hat{\theta}_{JS} $ performs poorly in a sparse setting is the case of the $r$-spike parameter value $ \theta_r$ with $r$ coordinates at $ \sqrt{n/r} $ which has $ \Vert \theta \Vert^2 =n $. \citet{johnstone2004needles} show that $ E \Vert \hat{\theta}^{JS} - \theta \Vert \leq n $ with risk $2$ at the origin. Moreover, 
\[
\frac{n \Vert \theta \Vert^2}{ n + \Vert \theta \Vert^2} \leq R \left ( \hat{\theta}^{JS} , \theta_r \right ) \leq
2 + \frac{n \Vert \theta \Vert^2}{ n + \Vert \theta \Vert^2}
\]
Hence for the $r$-spike parameter value $ R \left( \hat{\theta}^{JS} , \theta_r \right) \geq (n/2)$. As \citet{donoho1994ideal} showed, simpler rules such as the hard-thresholding and soft-thresholding estimates given by $\hat{\theta}^{H}(y,\lambda) = y I\{\abs{y} \ge \lambda \}$ and $\hat{\theta}^{S}(y,\lambda) = sgn(y) (\abs{y} - \lambda)_{+}$ satisfy an Oracle inequality. In particular, when the thresholding sequence is close to $\sqrt{2\log n}$ (`universal threshold'), these estimators attain the ``oracular risk'' up to a factor of $2\log(n)$. 

\paragraph{Near minimax $\ell_2$ risk}

The asymptotically minimax risk rate in $\ell_2$ for nearly black objects is given by \citet{donoho1992maximum} to be $p_n \log \left ( n / p_n \right )$. Here $a_n \asymp b_n$ means $\lim_{n\to\infty} a_n/b_n=1$. Specifically, for any estimator $\delta(y)$, we have a lower bound: 
\beq
\sup_{\theta_0 \in \ell_0[p_n]} \E_{\theta_0} \norm{\delta(Y) - \theta_0}^2 \ge 2 \sigma^2 p_n \log(n/p_n)(1+o(1))
\eeq
The minimax rate, which is a frequentist criteria for evaluating the convergence of point estimators to the underlying true parameter, is a validation criteria for posterior contraction as well. This result is due to \citet{ghosal2000} who showed that the minimax rate is the fastest that the posterior distribution can contract. 

The horseshoe estimators also enjoy `near-minimax' rates in both an empirical Bayes and full Bayes approach, provided that the hyper-parameters or the priors are suitably chosen - as proved in a series of papers \citep{van2014horseshoe,van2015conditions,van2016many,van2017adaptive}. Specifically, the horseshoe estimator achieves
\beq
\sup_{ \theta \in l_0[p_n] } \; \mathbb{E}_{ y | \theta } \norm{ \estHs (y) - \theta }^2 \asymp p_n \log \left ( n / p_n \right ), \label{eq:minimax}
\eeq
\citet{van2014horseshoe} showed that the near-minimax rate can be achieved by setting the global shrinkage parameter $\tau = (p_n/n) \log(n/p_n)$. In practice, $\tau$ is unknown and must either be estimated from the data or handled via a fully Bayesian approach by putting a suitable prior on $\tau$.  \cite{van2017adaptive} show that the theoretical optimality properties for the popular horseshoe prior holds true if the global shrinkage parameter $\tau$ is learned via the maximum marginal likelihood estimator (MMLE) or a full Bayes approach. For the full Bayes estimator, these conditions are easily seen to satisfied by a half-Cauchy prior truncated to the interval $[1/n,1]$, which also does well in numerical experiments, both in `sparse' and `less-sparse' situations. Independently, \citet{van2015conditions} and \citet{ghosh2016asymptotic} showed that these optimality properties are not unique features of the horseshoe prior and they hold for a general class of global-local shrinkage priors. While \cite{van2015conditions}'s results apply for a wider class of priors, including the horseshoe+ prior \citep{bhadra2015horseshoe+} and spike-and-slab Lasso \citep{rovckova2016spike}, \citet{ghosh2016asymptotic} attain sharper mean-squared error bounds. 

\paragraph{Lasso}

A natural question is how does the Lasso fare in these aspects? While the MAP estimator under the Bayesian formulation of Lasso (i.e. iid Laplace prior on $\theta_i$'s) enjoys all the desirable properties of the frequentist Lasso, it is known to be sub-optimal for recovery of the underlying $\btheta_0$ \citep{castillo2012needles}. In fact, \cite{castillo2012needles} show that unlike the mode, the full posterior distribution under the Laplace prior does not contract at the optimal rate, making it `useless for \textit{uncertainty quantification}'. 

%\begin{enumerate}
%\item  \citet{van2014horseshoe} showed it for horseshoe.
%\item  \citet{van2015conditions} showed it for horseshoe+ and several other ``global-local'' models.
%\item \citet{ghosh14} is similar to  \citet{van2015conditions}.
%\item \citet{van2016many} is a new paper that we need to read and possibly cite. 
%\end{enumerate}

\paragraph{Asymptotic Bayes Optimality under Sparsity}

One of main reasons behind the widespread popularity of Lasso is its in-built mechanism for performing simultaneous shrinkage and selection. The frequentist Lasso or the equivalent MAP estimator under i.i.d Laplace priors induces automatic sparsity and can be easily adjusted to achieve model selection consistency. The horseshoe estimator, on the other hand, is a shrinkage rule that induces a selection rule through thresholding the pseudo posterior inclusion probabilities. Thus, we can compare their relative performance for multiple testing under the two-groups model and a $0$-$1$ additive loss framework. It turns out that for large scale testing problems the horseshoe prior attains the `oracle' property while the Laplace tails prove to be insufficiently heavy leading to a higher misclassification rate compared to the Horseshoe prior. The main reasons behind horseshoe prior's optimality are the posterior density of shrinkage weights that can pushes most of the density to $0$ and $1$ and the adaptability of the global shrinkage paramater $\tau$. 

The posterior distribution under the horseshoe prior leads to a natural model selection strategy under the two-groups model. \citet{carvalho2010horseshoe} argued that the shrinkage coefficient $1-\hat{\kappa}_i$ can be interpreted as a pseudo-inclusion probability $P(\theta_i \ne 0 \mid y_i)$, and it induces a multiple testing rule.
\beq
\text{Reject the $i^{th}$ null hypothesis } H_{0i} : \theta_i = 0 \text{ if } 1-\hat{\kappa}_i > \half \label{eq:hsrule}
\eeq
Under the two-groups model \eqref{twogroups}, and a $0$-$1$ loss, the Bayes risk is 
\[
R = \sum_{i=1}^{n} \{ (1- \pi) t_{1i} + \pi t_{2i} \}
\]

If we know the true values of the sparsity and the parameters of the non-null distribution, we can derive a decision rule that is impossible to beat in practice, this is called the Bayes Oracle for multiple testing \citep{bogdan2011asymptotic}. The ``oracular risk'' serves as the lower bound for any multiple testing rule under the two-groups model and thus provides an asymptotic optimality criteria when the number of tests go to infinity. The asymptotic framework of \citet{bogdan2011asymptotic} is 
\beq
p_n \to 0, \; u_n = \psi_n^2 \to \infty, \; \text{and} \; \log(v_n)/u_n \to C \in (0,\infty) \label{eq:asymp}
\eeq
where $v_n = \psi_n^2 (\frac{1-p_n}{p_n})^2$. The Bayes risk for the Bayes oracle under the above framework \eqref{eq:asymp} is given by:
\[
R_{\text{Oracle}} = n \pi (2 \Phi(\sqrt{C}) - 1)(1+o(1))
\]
A multiple testing rule is called ABOS (asymptotic Bayes optimal under sparsity) if it attains the oracular risk as $n \to \infty$. \citet{bogdan2011asymptotic} provided conditions for a few popular testing rules, e.g. Benjamini--Hochberg FDR controlling rule to be ABOS. \citet{datta2013asymptotic} first showed that the decision rule \eqref{eq:hsrule} is also ABOS up to a multiplicative constant if $\tau$ is chosen suitably to reflect the sparsity, i.e. $\tau = O(\pi)$. \citet{datta2013asymptotic}'s proof hinges on the concentration of posterior distribution near $0$ or $1$, depending on the trade-off between signal strength and sparsity. In their numerical experiments, \citet{datta2013asymptotic} also confirmed the idea that the horseshoe prior induced rule outperforms the shrinkage rule induced by the Laplace prior under different levels of sparsity. 
Although $\tau$ is treated as a tuning parameter that mimics $\pi$ in the theoretical treatment, in practice, $\pi$ is an unknown parameter. Several authors \cite{datta2013asymptotic, ghosh2016asymptotic, ghosh2016testing,van2016many} have shown that usual estimates of $\tau$ adapts to sparsity, a condition that also guarantees near-minimaxity in estimation. \citet{ghosh2016testing} extended the ABOS property to a wider class of global-local shrinkage priors, with conditions on the slowly varying tails of the local shrinkage prior. They have also shown that the testing rule under a horsehsoe-type prior is \textit{exactly} ABOS, when $\lim_{n \to \infty} \tau/p \in (0, \infty)$. 


%\subsection{Prediction using global-local priors}
%\begin{enumerate}
%\item \citet{ carvalho2010horseshoe}: K-L superefficiency for predictive density for horseshoe.
%\end{enumerate}

\section{Hyper-parameter Tuning}

\paragraph{Optimization and Cross-validation}

Careful handling of the global shrinkage parameter $\tau$ is critical for sucess of the horseshoe estimator in a sparse regime as it captures the level of sparsity in the data \citep{carvalho2010horseshoe, datta2013asymptotic, van2015conditions, van2016many}. However, in nearly black situation a naive estimate of $\tau$ could collapse to zero, and care must be taken to prevent possible degenaracy in inference. There are two main approaches regarding choice of $\tau$: first, a fully Bayesian approach that specifies a hyper-prior on $\tau$ and second, an empirical Bayesian approach that estimates $\tau$ from the data using a simple thresholding or maximum marginal likelihood approach (MMLE). In a recent paper, \citet{van2017adaptive} have investigated the empirical Bayes and full Bayes approach for $\tau$, and have shown that the full Bayes and the MMLE estimator achieve the near minimax rate ($p_n \log(n)$) under similar conditions. For the full Bayes estimator, these conditions are easily seen to satisfied by a half-Cauchy prior truncated to the interval $[1/n,1]$, which also does well in numerical experiments, both in `sparse' and `less-sparse' situations. The MMLE estimator of \citet{van2017adaptive} outperforms the simple thresholding estimator given by:
\[
\hat{\tau}_s(c_1, c_2) = \max \left \{ \frac{\sum_{i=1}^{n} \1 \{ \abs{y_i} \ge \sqrt{c_1 \log(n) \}}}{c_2 n}, \frac{1}{n} \right\}
\]
More importantly, the MMLE estimator can detect smaller non-zero signals, even those below the threshold $\sqrt{2 \log(n)}$, such as $\theta_i = 1$ when $n = 100$. 
The success of the MMLE estimator, both theoretically and numerically, challenges the notion that for the horseshoe prior an empirical Bayes parameter estimate of $\tau$ cannot replace a full Bayes estimate of $\tau$. Of course, one needs care to prevent the estimator to be too close to zero. 

A third approach could be treating $\tau$ as a tuning parameter and using a $k$-fold cross-validation to select $\tau$. Like the full Bayes and empirical Bayes approach, the cross-validated choice of $\hat{\tau}$ can also converge to zero and care should taken to avoid zero in such situations. Yet another approach for handling $\tau$ was proposed by \citet{piironen2016hyperprior}, who have investigated the choice of $\tau$ for a linear regression model and have suggested to choose the prior for $\tau$ by studying the prior for $m_{\text{eff}} = \sum_{i=1}^{n} (1-\kappa_i)$, the effective number of non-zero parameters. 

%Path (computational speed), Sensitivity Analysis. Cross-validation approach (has caveats), marginal MLE, $\argmax_{\tau} p(y \mid \tau)$. Plug-in estimators can be inadmissible, SURE: procedure. 

\paragraph{Marginal Likelihood: } We take a closer look at how $\tau$ affects the marginal likelihood under the horseshoe prior and the maximum marginal likelihood approach of \cite{van2017adaptive}. 
We can write the marginal likelihood under the horsehoe prior in \eqref{eq:hs} after marginalising out $\theta_i$ from the model as:
\beq
m(y \mid \tau) = \prod_{i=1}^{n} (1+\lambda_i^2 \tau^2)^{-\half} \exp \left \{ - \frac{y_i^2}{2(1+\lambda_i^2 \tau^2)} \right \}  (1+\lambda_i^2)^{-1} d \lambda_i 
\eeq
As \citet{tiao1965bayesian} point out, the marginal likelihood is positive at $ \tau = 0$. Hence the impropriety of the prior of $\tau^{-2} $ at the origin translates to the posterior. As a result, a maximum likelihood estimator of $\tau$ has a potential danger of collapsing to zero in very sparse problems \citep{polson2010shrink, datta2013asymptotic}. In \cite{van2017adaptive}'s approach, both the empirical Bayes MMLE and the full Bayes solution are restricted in the interval $[1/n,1]$ to pre-empt this beaviour. To get the MMLE of $\tau$ in \cite{van2017adaptive}'s approach,  we first calculate the marginal prior of $\theta_i$ after integrating out $\lambda_i^2$ in \eqref{eq:hs}:
\beq
p_{\tau}(\theta_i) = \int_{0}^{\infty} \frac{1}{\sqrt{2 \pi}} \exp \left\{ - \frac{\theta_i^2}{2\lambda^2 \tau^2} \right\} \frac{1}{\lambda \tau} \frac{2}{\pi(1+\lambda^2)} d\lambda
\eeq
The MMLE is then obtained as the maximizer of the marginal likelihood restricted to the interval $[1/n,1]$: 
\beq
\hat{\tau}_M = \argmax_{\tau \in [1/n,1]} \prod_{i=1}^{n} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} \exp \left\{ - \frac{(y_i -\theta_i)^2}{2} \right\} p_{\tau}(\theta_i) d\theta_i
\eeq
The lower bound of the maximization interval prevents against a degenerate solution of $\tau$ in sparse case. 

%The marginal likelihood is decreasing at zero when the $S_i$'s are small
%enough to make the exponential term nearly constant \citep{tiao1965bayesian}. This is precisely the sparse coefficient case.
%
%A number of default choices have been proposed to overcome this issues.
%\citet{morris2011estimating} propose a flat prior $ p( \tau ) \equiv 1 $. The tails of the likelihood are sufficient so as to lead to a proper posterior. This is also related to Stein's harmonic prior $ || \theta ||^{-(k-2)} $ for $ k \geq 3 $.  

%Methods for choosing $\tau$ will involve minimizing some criteria: %such as AIC \citep{akaike74}, BIC \citep{schwarz1978}, DIC \citep{spiegelhalter2002bayesian}, SURE \citep{stein81}, $C_p$ \citep{mallows73}. \citet{castillo2015bayesian} compared full Bayes MCMC with empirical Bayes and plug-in approaches.

\section{Computation and Simulation}

Over the last few years, several different implementation of the horseshoe prior for normal means and regression model has been proposed. The MCMC based implementations usually proceed via block-updating $\btheta$, $\blambda$ and $\tau$ using either a Gibbs or parameter expansion or slice sampling strategy. The first \textsc{R} package to offer horseshoe prior for regression along with Lasso, Bayesian Lasso and Ridge was the \texttt{monomvn} package by \citet{gramacy2010shrinkage}. In an unpublished technical report, \citet{scott_parameter_2010} proposed a parameter expansion strategy for the horseshoe prior and studied its effect on the autocorrelation of $\tau$. Furthermore, \citet{scott_parameter_2010} pointed out that the solution to this lies in marginalizing over the local shrinkage parameter $\lambda_j$'s. On a somewhat similar route, \citet{makalic2016high} uses a inverse-gamma scale mixture identity to construct a Gibbs sampling scheme for horseshoe and horseshoe+ prior for linear regression as well as logistic and negative binomial regression. The \texttt{horseshoe} package implements the MMLE and truncated prior approaches for handling $\tau$ proposed in \citet{van2017adaptive}. \citet{hahn_elliptical_2016} proposed an elliptical slice sampler and argues that it wins over Gibbs strategies for higher dimensional problems both in per-sample speed and quality of samples (i.e. effective sample size). The state-of-the-art implementation for horseshoe prior in linear regression is \citet{bhattacharya_fast_2015} who used a Gaussian sampling alternative to the na\''ive Cholesky decomposition to reduce the computational burden from $O(p^3)$ to $O(n^p)$. A very recent paper by \citet{james2017scalable} claims to improve this even further by implementing a block update strategy but using a random walk Metropolis--Hastings algorithm on $\log(1/\tau^2)$ for block-updating $\tau \mid \lambda$. We provide a list of all the implementations known to us on Table \ref{tab:hs-imp}.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Implementations of Horseshoe and Other Shrinkage Priors}
  \footnotesize{
    \begin{tabular}{|c|c|}
    \hline
    Implementation (Package/URL) & Authors \bigstrut\\
    \hline
    \textsc{R} package: \href{https://cran.r-project.org/web/packages/monomvn/index.html}{\texttt{monomvn}} & \citet{gramacy2010shrinkage} \bigstrut[t]\\
     \textsc{R} code in paper & \citet{scott_parameter_2010} \\
    \textsc{R} package: \href{https://cran.r-project.org/web/packages/horseshoe/index.html}{\texttt{horseshoe}} & \citet{pas_horseshoe:_2016} \\
    \textsc{R} package: \href{https://cran.r-project.org/web/packages/fastHorseshoe/index.html}{\texttt{fastHorseshoe}} & \citet{hahn_elliptical_2016} \\
    \href{https://github.com/antik015/Fast-Sampling-of-Gaussian-Posteriors}{\textsc{Matlab} code} & \citet{bhattacharya_fast_2015} \\
    GPU accelerated Gibbs sampling & \citet{terenin_gpu-accelerated_2016} \\
    \href{https://cran.r-project.org/web/packages/bayesreg/index.html}{\texttt{bayesreg}} + \textsc{Matlab} code in paper & \citet{makalic2016high} \\
     \href{https://github.com/jamesjohndrow/horseshoe_jo}{\textsc{Matlab} code} & \citet{james2017scalable} \bigstrut[b]\\ 
    \hline
    \end{tabular}%
    }
  \label{tab:hs-imp}%
\end{table}%


%\begin{enumerate}
%\item Mention R package. 
%\item Compare with various methods.
%\end{enumerate}


\section{Applications and Extensions}\label{sec:app-ext}

%Literature review of applications and extensions and recent development. List papers. 
%Fused and group lasso. Extension to logistic (Polya-Gamma for logistic : MCMC still convex). 

\paragraph{Applications and Extensions of Lasso:} Since the inception of Lasso as a regularisation method for linear regression in 1996, a great deal of extensions and applications have been proposed in the literature. The combined effect of convex penalty and sparsity of the final solution lead to huge computational gains by using powerful convex optimization methods on problems of massive dimensions. The coordinate descent approach \citep{friedman2007pathwise,friedman2010regularization} is one particularly promising approach, that works by applying soft-threshold to the least-sqaures solution obtianed on partial residuals, one at a time. The coordinate descent approach is flexible and easy and can be proved to converge to the solution as long as the log-likelihood and penalty are convex \citep{tseng2001convergence}, paving the way for wide applicability of $\ell_1$ penalty in generalized linear models (GLM). The popoular R package \texttt{glmnet} provides a nice and easy interface for applying lasso and elastic-net penalty for a general sparse GLM. Although a comprehensive list of regularization methods that extend the idea of Lasso and even move beyond the convex penalty is beyond the scope of this article, we give a list of popoular regularization methods in Table \ref{tab:lasso:ext}, which is adapted from \citet{tibshirani2014praise}:

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[ht!]
  \centering
  \caption{A few regularization methods}
  \footnotesize{
    \begin{tabular}{|c|c|}
    \hline
    Method  & Authors  \bigstrut\\
    \hline
    Adaptive Lasso & \citet{zou2006adaptive} \bigstrut[t]\\
    Compressive sensing  & \citet{donoho2006compressed,candes2008restricted} \\
    Dantzig selector  & \citet{candes2007dantzig} \\
    Elastic net & \citet{zou2005regularization} \\
    Fused lasso & \citet{tibshirani2005sparsity} \\
    Generalized lasso & \citet{tibshirani2011solution} \\
    Graphical lasso & \citet{friedman2008sparse} \\
    Grouped lasso & \citet{yuan2006model} \\
    Hierarchical interaction models & \citet{bien2013lasso} \\
    Matrix completion & \citet{candes2010power,mazumder2010spectral} \\
    Multivariate methods & \citet{jolliffe2003modified,witten2009penalized} \\
    Near-isotonic regression & \citet{tibshirani2011nearly} \\
    Square Root Lasso  & \citet{belloni2011square} \\
    Scaled Lasso & \citet{sun2012scaled} \\
    Minimum concave penalty & \citet{zhang2010nearly} \\
    SparseNet & \citet{mazumder2012} \bigstrut[b]\\
    \hline
    \end{tabular}%
    }
  \label{tab:lasso:ext}%
\end{table}%

\paragraph{Applications and Extensions of Horseshoe:}

As discussed in \S \ref{sec:one-gp}, the Horseshoe prior belongs to a wider class of global-local shrinkage priors \citep{polson2010shrink} that are characterized by a local shrinkage parameter for recovering large signals and a global shrinkage parameter for adapting to overall sparsity. The class of global-local priors, although differing in their specific goals and design, exhibit some common features: heavy tails for tail-robustness and appreciable mass near zero for sparsity, leading to shared optimality properties. Several authors including \citet{van2015conditions, ghosh2016asymptotic, ghosh2016testing} have provided conditions for optimality of one-group continuous priors for estimation of sparse normal means and multiple testing. Table \ref{tab:one-gps} provides a sampling of a few continuous shrinkage priors popular in the literature. 

Although the original horseshoe prior was developed for signal recovery with sparse Gaussian means problem , the idea of directly modeling the posterior inclusion probability and use of normal-scale mixture to facilitate sparsity is a flexible idea and can be easily generalized to a wider class of problems. \citet{bhadra2015default} show that the horseshoe prior is a good candidate as a default prior for low-dimensional, possibly non-linear functionals of high-dimensional paramater and can resolve long-standing marinalization paradoxes for such problems. \citet{bhadra2016prediction} show how to use global-local priors for prediction and provide theoretical and numerical evidence that it performs better than a variety of competitors including lasso, ridge, PCR and sparse PLS. 

Moving beyond Gaussianity, \citet{datta2016bayesian} re-discovered the Gauss-hypergeometric prior for flexible shrinkage needed for quasi-sparse count data, with a tighter control on false discoveries. \citet{piironen2016hyperprior} used a Gaussian approximation using a second-order Taylor expansion for the log-likelihood to apply the horseshoe prior for the generalized linear model. \citet{wang2013class} proposed a shrinkage prior based on a scale mixture of uniform for covariance matrix estimation. \citet{peltola2014hierarchical} applies the horseshoe prior for Bayesian linear survival regression for selecting covariates with highest predictive values. Given the explosive growoth of the methods in this area, we conjecture that the horseshoe prior would be regarded as a key tool sparse signal recovery and as a default prior for objective Bayesian inference for many important problems. 

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{A few global-local shrinkage priors}
  \footnotesize{
    \begin{tabular}{|c|c|}
    \hline
    Global-local shrinkage prior  & Authors  \bigstrut\\
    \hline
    Normal Exponential Gamma & \citet{griffin2005alternative} \bigstrut[t]\\
    Horseshoe & \citet{carvalho2010horseshoe, carvalho2009handling} \\
    Hypergeometric Inverted Beta & \citet{polson2010large} \\
    Generalized Double Pareto & \citet{armagan2011generalized} \\
    Generalized Beta  & \citet{armagan2013generalized} \\
    Dirichlet-Laplace & \citet{bhattacharya2014dirichlet} \\
    Horseshoe+  & \citet{bhadra2015horseshoe+} \\
    Horseshoe-like & \citet{bhadra2017horseshoe} \\
    Spike-and-Slab Lasso & \citet{rovckova2016spike} \\
    R2-D2 & \citet{zhang2016high} \bigstrut[b]\\
    \hline
    \end{tabular}%
    }
  \label{tab:one-gps}%
\end{table}%


%\begin{enumerate}
%\item \citet{bhadra2015default} use global-local priors in default Bayes Efron problems.
%\item \citet{bhadra2016prediction} show how to use global-local priors for prediction. Performs better than a variety of competitors including lasso, ridge, PCR and sparse PLS.
%\item \citet{datta2015inference} use global-local priors to model the rate parameter for Poisson count data.
%\end{enumerate}


\section{Discussion}

\textcolor{red}{
What's left to do? Horseshoe subset selection. Lasso computationally quick / scalable. Horseshoe needs MCMC etc. 
\begin{enumerate}
\item So many global-local priors; what is the unifying theme? Spike and heavy tails \citet{polson2010shrink,van2015conditions}.
\item How close to minimax constant can we get in estimation? Only near-minimaxity and the best constant is $4\sigma^2$ \citep{van2017adaptive}.
\item How close to oracle risk can we get in testing? We can get exact rate \citep{ghosh2016testing}.
\item Is prediction performance optimal? 
\item Yet to rigorously show global-local priors have any information theoretic properties in default Bayes problems that reference priors \citep{bernardo1979reference} enjoy.
\item \citet{bhadra2016global} demonstrate how global-local mixtures can be generated using two integral identities. This might prove useful in EM and MCMC.
\end{enumerate}
}

\begin{appendix}

%\section{Appendix: Algorithms}

%Literature review of algorithms and R packages. 
%LASSO: glmnet, genlasso. 
%Horseshoe: horseshoe, fastHorseshoe, monomvn, our own package. 

%\section*{Appendix}
%Possibly give some R code vignette.

%\section*{Other refs}
%The 1988 Neyman Memorial Lecture: A Galtonian Perspective on Shrinkage Estimators - Stephen M. Stigler

\section{Two-groups Model}\label{sec:2gp}

The two-groups model is a natural hierarchical Bayesian solution to the sparse signal-recovery problem, that is ``almost as simple to describe as the problem".  The two-groups solution to the signal detection problem is as follows:
\ben
\item Assume each $\theta_i$ is non-zero with some common prior probability $(1 - \pi)$, and that the nonzero $\theta_i$ come from a commonn density $\Nor(0,\psi^2)$. 
\item Calculate the posterior probabilities that each $y_i$ comes from $\Nor(0,\psi^2)$. 
\een
The most important aspect of this model is that it automatically adjusts for multiplicity without any ad-hoc regularization, i.e. it lets the data choose $\pi$ and then carry out the tests on the basis of the posterior inclusion probabilities $\omega_i = P(\theta_i \neq 0 | y_i)$. Formally, in a two-groups model $\theta_i$'s are modeled as
\beq
\theta_i | \pi, \psi = (1-\pi)\delta_{\{0\}} + \pi \Nor (0, \psi^2), \label{spikeslab}
\eeq
where $\delta_{\{0\}}$ denotes a point mass at zero and the parameter $\psi^2>0$ is the non-centrality parameter that determines the separation between the two groups. Under this setting, the marginal distribution of $(y_i \mid \pi, \psi)$ is given by
\beq
y_i \mid \pi, \psi \sim  (1-\pi) \Nor(0, 1) + \pi \Nor(0, 1+\psi^2). \label{twogroups}
\eeq
%For the theoretical discussion, we will confine ourselves to the case where $\sigma_0^2 = 1$, a common assumption for multiple testing asymptotics (see Abramovich et al. (2006) and \cite{bogdan2008comparison,bogdan2011asymptotic} for example). 
As can be seen from \eqref{twogroups}, the two-groups model leads to a sparse estimate, i.e., it puts exact zeros in the model. 
\end{appendix}


\bibliographystyle{plainnat}
\bibliography{hs-review}

\end{document}
